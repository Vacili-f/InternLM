### 第四节课笔记

1. LLM的两种微调方式：增量预训练和指令跟随

   1. 指令跟随微调：对数据处理后仅对output部分计算损失，可以理解为一种对**对话模式**的进一步学习
   2. 增量预训练：将全部输入数据作为output即可，仍对output计算损失

2. NTK：LoRA, QLoRA

3. 使用xtuner的流程简单总结：

   1. 准备配置文件（xtuner提供了一些）

      ```shell
      # 列出所有提供的配置文件
      xtuner list-cfg
      ```

   2. 模型下载

   3. 数据集下载

   4. 开始微调

      1. 微调后得到huggingface adapter，可以将adapter合并到基座模型

4. 赋予LLM以Agent能力

   1. 使LLM具备调用在线插件等能力，不仅仅依赖于模型学习到的知识